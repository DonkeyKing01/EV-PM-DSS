"""
ChromaDB Vector Database Builder for EV PM DSS
å‘é‡æ•°æ®åº“æ„å»ºè„šæœ¬ - å¤„ç†å¹¶åµŒå…¥ UGCã€è½¦å‹è§„æ ¼å’Œç”¨æˆ·ç”»åƒæ•°æ®

Author: EV PM DSS Team
Date: 2026-02-15
"""

import os
import json
import pandas as pd
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import logging
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ==================== Configuration ====================
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "Data")
VECTOR_DB_PATH = os.path.join(DATA_DIR, "Vector")

# æ•°æ®æ–‡ä»¶è·¯å¾„
UGC_FILE = os.path.join(DATA_DIR, "Processed", "ugc.csv")
CONFIG_FILE = os.path.join(DATA_DIR, "Processed", "vehicles_config.json")
PERSONA_FILE = os.path.join(DATA_DIR, "Analyzed", "Persona", "step4_user_persona_full.csv")

# ==================== Logging ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('vector_build.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class VectorDBBuilder:
    """ChromaDB å‘é‡æ•°æ®åº“æ„å»ºå™¨"""
    
    def __init__(self, embedding_model: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        """
        åˆå§‹åŒ–æ„å»ºå™¨
        
        Args:
            embedding_model: SentenceTransformer æ¨¡å‹åç§°
                - 'all-MiniLM-L6-v2': è½»é‡çº§è‹±æ–‡æ¨¡å‹ (384ç»´)
                - 'paraphrase-multilingual-MiniLM-L12-v2': å¤šè¯­è¨€æ¨¡å‹ (384ç»´)
                - 'distiluse-base-multilingual-cased-v2': å¤šè¯­è¨€é«˜æ€§èƒ½ (512ç»´)
        """
        logger.info(f"Initializing VectorDBBuilder with model: {embedding_model}")
        
        # åˆå§‹åŒ– Embedding æ¨¡å‹
        self.embedding_model = SentenceTransformer(embedding_model)
        logger.info(f"Embedding dimension: {self.embedding_model.get_sentence_embedding_dimension()}")
        
        # åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(
            path=VECTOR_DB_PATH,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        logger.info(f"ChromaDB initialized at: {VECTOR_DB_PATH}")
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """ç”Ÿæˆæ–‡æœ¬çš„å‘é‡åµŒå…¥"""
        return self.embedding_model.encode(texts, show_progress_bar=True).tolist()
    
    def build_ugc_collection(self, limit: int = None):
        """
        æ„å»º UGC è¯„è®ºå‘é‡é›†åˆ
        
        Args:
            limit: é™åˆ¶å¤„ç†çš„è¯„è®ºæ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
        """
        logger.info("=" * 70)
        logger.info("Building UGC Reviews Vector Collection")
        logger.info("=" * 70)
        
        # åˆ›å»ºæˆ–è·å–é›†åˆ
        collection = self.client.get_or_create_collection(
            name="ugc_reviews",
            metadata={"description": "User-generated content reviews"}
        )
        
        # è¯»å–æ•°æ®
        logger.info(f"Loading UGC data from {UGC_FILE}")
        df = pd.read_csv(UGC_FILE)
        
        if limit:
            df = df.head(limit)
            logger.info(f"Limited to {limit} reviews for testing")
        
        logger.info(f"Total reviews to process: {len(df)}")
        
        # å‡†å¤‡æ•°æ®
        documents = []
        metadatas = []
        ids = []
        
        dimensions = ['appearance', 'interior', 'space', 'intelligence', 'driving', 'range', 'value']
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing reviews"):
            review_id = str(row['review_id'])
            row_idx = int(idx)  # ä½¿ç”¨è¡Œç´¢å¼•ç¡®ä¿å”¯ä¸€æ€§
            
            # 1. å…¨æ–‡è¯„è®ºï¼ˆå¦‚æœæœ‰ç»¼åˆè¯„è®ºå­—æ®µï¼‰
            # è¿™é‡Œæˆ‘ä»¬ç»„åˆæœ€æ»¡æ„å’Œæœ€ä¸æ»¡æ„ä½œä¸ºæ‘˜è¦
            summary_parts = []
            if pd.notna(row.get('most_satisfied')):
                summary_parts.append(f"æœ€æ»¡æ„: {row['most_satisfied']}")
            if pd.notna(row.get('least_satisfied')):
                summary_parts.append(f"æœ€ä¸æ»¡æ„: {row['least_satisfied']}")
            
            if summary_parts:
                full_text = " | ".join(summary_parts)
                documents.append(full_text)
                metadatas.append({
                    "review_id": review_id,
                    "type": "summary",
                    "brand": str(row['brand']) if pd.notna(row['brand']) else "",
                    "series": str(row['series']) if pd.notna(row['series']) else "",
                    "model": str(row['model']) if pd.notna(row['model']) else "",
                    "date": str(row['review_date']) if pd.notna(row['review_date']) else "",
                    "location": str(row['purchase_location']) if pd.notna(row['purchase_location']) else ""
                })
                ids.append(f"review_{row_idx}_summary")
            
            # 2. åˆ†ç»´åº¦è¯„è®º
            for dim in dimensions:
                review_col = f'{dim}_review'
                score_col = f'{dim}_score'
                
                review_text = row.get(review_col)
                if pd.notna(review_text) and str(review_text).strip():
                    documents.append(str(review_text))
                    metadatas.append({
                        "review_id": review_id,
                        "type": "dimension_review",
                        "dimension": dim,
                        "score": float(row[score_col]) if pd.notna(row.get(score_col)) else None,
                        "brand": str(row['brand']) if pd.notna(row['brand']) else "",
                        "series": str(row['series']) if pd.notna(row['series']) else "",
                        "model": str(row['model']) if pd.notna(row['model']) else ""
                    })
                    ids.append(f"review_{row_idx}_{dim}")
        
        logger.info(f"Prepared {len(documents)} text documents for embedding")
        
        # æ‰¹é‡åµŒå…¥å’Œå­˜å‚¨
        batch_size = 5000
        for i in range(0, len(documents), batch_size):
            batch_docs = documents[i:i+batch_size]
            batch_meta = metadatas[i:i+batch_size]
            batch_ids = ids[i:i+batch_size]
            
            logger.info(f"Processing batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}")
            embeddings = self.embed_texts(batch_docs)
            
            collection.add(
                documents=batch_docs,
                embeddings=embeddings,
                metadatas=batch_meta,
                ids=batch_ids
            )
        
        logger.info(f"âœ… UGC collection built successfully! Total documents: {collection.count()}")
    
    def build_vehicle_specs_collection(self):
        """æ„å»ºè½¦å‹è§„æ ¼å‘é‡é›†åˆ"""
        logger.info("=" * 70)
        logger.info("Building Vehicle Specs Vector Collection")
        logger.info("=" * 70)
        
        collection = self.client.get_or_create_collection(
            name="vehicle_specs",
            metadata={"description": "Vehicle specifications and features"}
        )
        
        # è¯»å–è½¦å‹é…ç½®
        logger.info(f"Loading vehicle config from {CONFIG_FILE}")
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            vehicles = json.load(f)
        
        logger.info(f"Total vehicles to process: {len(vehicles)}")
        
        documents = []
        metadatas = []
        ids = []
        
        for vehicle in tqdm(vehicles, desc="Processing vehicles"):
            model_name = vehicle['model']
            
            # æ„å»ºæè¿°æ€§æ–‡æœ¬
            desc_parts = [
                f"è½¦å‹: {model_name}",
                f"å“ç‰Œ: {vehicle['brand']}",
                f"è½¦ç³»: {vehicle['series']}",
                f"ä»·æ ¼: {vehicle.get('price', 'N/A')}ä¸‡å…ƒ"
            ]
            
            # æ·»åŠ å…³é”®é…ç½®
            if vehicle.get('battery'):
                battery = vehicle['battery']
                desc_parts.append(f"ç”µæ± : {battery.get('capacity', 'N/A')}kWh {battery.get('type', '')}")
                desc_parts.append(f"ç»­èˆª: {battery.get('cltc_range', 'N/A')}km")
            
            if vehicle.get('intelligence'):
                intel = vehicle['intelligence']
                if intel.get('cockpit_system'):
                    desc_parts.append(f"åº§èˆ±ç³»ç»Ÿ: {intel['cockpit_system']}")
                if intel.get('adas_system'):
                    desc_parts.append(f"æ™ºé©¾ç³»ç»Ÿ: {intel['adas_system']}")
                if intel.get('lidar_count', 0) > 0:
                    desc_parts.append(f"æ¿€å…‰é›·è¾¾: {intel['lidar_count']}ä¸ª")
            
            if vehicle.get('performance'):
                perf = vehicle['performance']
                if perf.get('acceleration_0_100'):
                    desc_parts.append(f"ç™¾å…¬é‡ŒåŠ é€Ÿ: {perf['acceleration_0_100']}ç§’")
            
            full_desc = " | ".join(desc_parts)
            
            documents.append(full_desc)
            metadatas.append({
                "model": model_name,
                "brand": vehicle['brand'],
                "series": vehicle['series'],
                "price": vehicle.get('price'),
                "category": vehicle.get('category', '')
            })
            ids.append(model_name.replace(' ', '_'))
        
        # åµŒå…¥å’Œå­˜å‚¨
        logger.info("Generating embeddings...")
        embeddings = self.embed_texts(documents)
        
        collection.add(
            documents=documents,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )
        
        logger.info(f"âœ… Vehicle specs collection built successfully! Total documents: {collection.count()}")
    
    def build_persona_collection(self):
        """æ„å»ºç”¨æˆ·ç”»åƒå‘é‡é›†åˆ"""
        logger.info("=" * 70)
        logger.info("Building Persona Vector Collection")
        logger.info("=" * 70)
        
        collection = self.client.get_or_create_collection(
            name="user_personas",
            metadata={"description": "User persona descriptions"}
        )
        
        # è¯»å–ç”»åƒæ•°æ®
        logger.info(f"Loading persona data from {PERSONA_FILE}")
        df = pd.read_csv(PERSONA_FILE)
        
        # è®¡ç®—ç”»åƒè´¨å¿ƒå’Œç»Ÿè®¡
        weight_cols = [c for c in df.columns if c.startswith('w_')]
        persona_stats = df.groupby('persona_name').agg({
            'review_id': 'count',
            **{col: 'mean' for col in weight_cols}
        }).reset_index()
        
        logger.info(f"Total personas to process: {len(persona_stats)}")
        
        documents = []
        metadatas = []
        ids = []
        
        dimension_names = {
            'w_appearance': 'å¤–è§‚',
            'w_interior': 'å†…é¥°',
            'w_space': 'ç©ºé—´',
            'w_intelligence': 'æ™ºèƒ½åŒ–',
            'w_driving': 'é©¾é©¶æ„Ÿå—',
            'w_range': 'ç»­èˆª',
            'w_value': 'æ€§ä»·æ¯”'
        }
        
        for _, row in persona_stats.iterrows():
            persona_name = row['persona_name']
            user_count = int(row['review_id'])
            
            # æ‰¾å‡º Top 3 å…³æ³¨ç»´åº¦
            weights = {dim: row[col] for col, dim in dimension_names.items()}
            top_dims = sorted(weights.items(), key=lambda x: x[1], reverse=True)[:3]
            
            # æ„å»ºæè¿°
            desc = f"ç”¨æˆ·ç”»åƒ: {persona_name} | ç”¨æˆ·æ•°é‡: {user_count} | "
            desc += "å…³æ³¨é‡ç‚¹: " + ", ".join([f"{dim}({weight:.2f})" for dim, weight in top_dims])
            
            documents.append(desc)
            metadatas.append({
                "persona_name": persona_name,
                "user_count": user_count,
                **{f"centroid_{dim}": float(row[col]) for col, dim in dimension_names.items()}
            })
            ids.append(persona_name.replace(' ', '_'))
        
        # åµŒå…¥å’Œå­˜å‚¨
        logger.info("Generating embeddings...")
        embeddings = self.embed_texts(documents)
        
        collection.add(
            documents=documents,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )
        
        logger.info(f"âœ… Persona collection built successfully! Total documents: {collection.count()}")
    
    def build_all(self, ugc_limit: int = None):
        """
        æ„å»ºæ‰€æœ‰å‘é‡é›†åˆ
        
        Args:
            ugc_limit: UGC è¯„è®ºæ•°é‡é™åˆ¶ï¼ˆæµ‹è¯•ç”¨ï¼‰
        """
        logger.info("ğŸš€ Starting Vector Database Construction")
        logger.info(f"Database path: {VECTOR_DB_PATH}")
        
        try:
            # 1. UGC è¯„è®º
            self.build_ugc_collection(limit=ugc_limit)
            
            # 2. è½¦å‹è§„æ ¼
            self.build_vehicle_specs_collection()
            
            # 3. ç”¨æˆ·ç”»åƒ
            self.build_persona_collection()
            
            logger.info("=" * 70)
            logger.info("âœ… All vector collections built successfully!")
            logger.info("=" * 70)
            
            # è¾“å‡ºç»Ÿè®¡
            logger.info("\nCollection Statistics:")
            for coll_name in ["ugc_reviews", "vehicle_specs", "user_personas"]:
                coll = self.client.get_collection(coll_name)
                logger.info(f"  - {coll_name}: {coll.count()} documents")
            
        except Exception as e:
            logger.error(f"Error during vector DB construction: {e}", exc_info=True)
            raise


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Build ChromaDB Vector Database")
    parser.add_argument(
        "--limit", 
        type=int, 
        default=None,
        help="Limit number of UGC reviews (for testing)"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="paraphrase-multilingual-MiniLM-L12-v2",
        help="Embedding model name"
    )
    
    args = parser.parse_args()
    
    builder = VectorDBBuilder(embedding_model=args.model)
    builder.build_all(ugc_limit=args.limit)
