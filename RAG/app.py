"""
EV PM DSS - Chainlit Frontend Application
ç”µåŠ¨æ±½è½¦äº§å“ç®¡ç†å†³ç­–æ”¯æŒç³»ç»Ÿ - Chainlit å‰ç«¯

Author: EV PM DSS Team
Date: 2026-02-15
"""

import chainlit as cl
from chainlit.input_widget import Select, Slider
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Add parent directory to path to enable imports
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
sys.path.insert(0, str(parent_dir))

# Import RAG tools
from RAG.tools import (
    VectorRetriever,
    GraphRetriever,
    HybridRetriever,
    QueryAnalyzer,
    format_ugc_context,
    format_vehicle_comparison,
    format_ipa_scores
)
from RAG.config import get_siliconflow_client

# Load environment variables
load_dotenv()

# ==================== Configuration ====================
APP_TITLE = "EV PM DSS | ç”µåŠ¨æ±½è½¦ PM å†³ç­–æ”¯æŒç³»ç»Ÿ"
APP_DESCRIPTION = """
æ¬¢è¿ä½¿ç”¨ **EV PM DSS** - åŸºäºçŸ¥è¯†å›¾è°±å’Œå‘é‡æ•°æ®åº“çš„æ™ºèƒ½å†³ç­–æ”¯æŒç³»ç»Ÿã€‚

### æ ¸å¿ƒåŠŸèƒ½
- ğŸ“Š **User Insights**: æ·±åº¦ç”¨æˆ·ç”»åƒåˆ†æ
- âš”ï¸ **Competitor Analysis**: æ™ºèƒ½ç«å“å¯¹æ¯”
- ğŸ“ **PRD Writer**: AI è¾…åŠ©éœ€æ±‚æ–‡æ¡£æ’°å†™

è¯·ä»å·¦ä¾§èœå•é€‰æ‹©åŠŸèƒ½æ¨¡å—å¼€å§‹ä½¿ç”¨ã€‚
"""

# Initialize tools (lazy loading)
vector_retriever = None
graph_retriever = None
hybrid_retriever = None
query_analyzer = None
llm_client = None


def get_tools():
    """è·å–å·¥å…·å®ä¾‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
    global vector_retriever, graph_retriever, hybrid_retriever, query_analyzer, llm_client
    
    if llm_client is None:
        llm_client = get_siliconflow_client()
    
    if vector_retriever is None:
        vector_retriever = VectorRetriever()
    if graph_retriever is None:
        graph_retriever = GraphRetriever()
    if hybrid_retriever is None:
        hybrid_retriever = HybridRetriever()
    if query_analyzer is None:
        query_analyzer = QueryAnalyzer(llm_client)
    
    return vector_retriever, graph_retriever, hybrid_retriever, query_analyzer, llm_client


# ==================== Chat Profiles (ä¾§è¾¹æ æ¨¡å—é€‰æ‹©) ====================
@cl.set_chat_profiles
async def chat_profile():
    """å®šä¹‰èŠå¤©é…ç½®æ–‡ä»¶ï¼ˆä¾§è¾¹æ æ˜¾ç¤ºï¼‰"""
    return [
        cl.ChatProfile(
            name="User Insights",
            markdown_description="ğŸ“Š **ç”¨æˆ·æ´å¯Ÿåˆ†æ**\n\nåŸºäºçŸ¥è¯†å›¾è°±çš„ 8 ä¸ªæƒå¨ç”¨æˆ·ç”»åƒï¼Œæ·±åº¦åˆ†æç”¨æˆ·éœ€æ±‚å’Œç—›ç‚¹ã€‚",
            icon="https://api.dicebear.com/7.x/shapes/svg?seed=user"
        ),
        cl.ChatProfile(
            name="Competitor Analysis",
            markdown_description="âš”ï¸ **ç«å“å¯¹æ¯”åˆ†æ**\n\næ··åˆæ£€ç´¢è½¦å‹å‚æ•°å’Œç”¨æˆ·è¯„è®ºï¼Œç”Ÿæˆä¸“ä¸šç«å“åˆ†ææŠ¥å‘Šã€‚",
            icon="https://api.dicebear.com/7.x/shapes/svg?seed=comp"
        ),
        cl.ChatProfile(
            name="PRD Writer",
            markdown_description="ğŸ“ **PRD æ–‡æ¡£æ’°å†™**\n\nå…¨æ ˆæ•°æ®é©±åŠ¨ï¼Œè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–äº§å“éœ€æ±‚æ–‡æ¡£ã€‚",
            icon="https://api.dicebear.com/7.x/shapes/svg?seed=prd"
        ),
    ]


# ==================== Startup ====================
@cl.on_chat_start
async def start():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    
    import time
    session_id = id(cl.user_session)
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»åˆå§‹åŒ–è¿‡ï¼ˆé¿å…é‡å¤æ˜¾ç¤ºæ¬¢è¿æ¶ˆæ¯ï¼‰
    already_initialized = cl.user_session.get("initialized", False)
    
    if already_initialized:
        print(f"\nâš ï¸ [ä¼šè¯ {session_id}] æ£€æµ‹åˆ°é‡æ–°åˆå§‹åŒ–ï¼Œè·³è¿‡æ¬¢è¿æ¶ˆæ¯\n")
        return
    
    print(f"\n{'='*80}")
    print(f"ğŸš€ [æ–°ä¼šè¯ {session_id}] å¼€å§‹")
    print(f"   æ—¶é—´: {time.strftime('%H:%M:%S')}")
    print(f"{'='*80}\n")
    
    # æ ‡è®°å·²åˆå§‹åŒ–
    cl.user_session.set("initialized", True)
    
    # è·å–å½“å‰é€‰æ‹©çš„æ¨¡å—
    chat_profile = cl.user_session.get("chat_profile")
    module_name = chat_profile if chat_profile else "User Insights"
    
    # åˆå§‹åŒ–ä¼šè¯å†å²
    cl.user_session.set("chat_history", [])
    cl.user_session.set("current_module", module_name)
    
    # å‘é€æ¬¢è¿æ¶ˆæ¯
    welcome_msg = f"""**æ¬¢è¿ä½¿ç”¨ EV PM DSS** ğŸš—

å½“å‰æ¨¡å—: **{module_name}**

æ‚¨å¯ä»¥éšæ—¶ä»å·¦ä¾§åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—éƒ½æœ‰ç‹¬ç«‹çš„å¯¹è¯å†å²ã€‚

---
{APP_DESCRIPTION}
"""
    
    await cl.Message(
        content=welcome_msg,
        author="System"
    ).send()
    
    # åˆå§‹åŒ–å·¥å…·
    try:
        get_tools()
        await cl.Message(
            content="âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå‘é‡åº“å’Œå›¾æ•°æ®åº“å·²è¿æ¥ã€‚",
            author="System"
        ).send()
    except Exception as e:
        await cl.Message(
            content=f"âš ï¸ ç³»ç»Ÿåˆå§‹åŒ–è­¦å‘Šï¼š{str(e)}\néƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ .env é…ç½®ã€‚",
            author="System"
        ).send()


def _build_history_text(max_rounds: int = 3) -> str:
    """æ„å»ºæœ€è¿‘ N è½®å¯¹è¯å†å²æ–‡æœ¬ï¼Œç”¨äºæ³¨å…¥ LLM promptã€‚
    
    æ¯æ¡æ¶ˆæ¯æˆªæ–­åˆ° 300 å­—ç¬¦ï¼Œé¿å…å†å²è¿‡é•¿å ç”¨ tokenã€‚
    å¦‚æœæ²¡æœ‰å†å²ï¼ˆç¬¬ä¸€è½®å¯¹è¯ï¼‰ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    chat_history = cl.user_session.get("chat_history", [])
    # æ’é™¤æœ€åä¸€æ¡ï¼ˆå³å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼Œå·²ç»ä½œä¸º user_query ä¼ å…¥ promptï¼‰
    history = chat_history[:-1] if chat_history else []
    if not history:
        return ""
    
    # å–æœ€è¿‘ max_rounds è½®ï¼ˆæ¯è½® = 1 user + 1 assistant = 2 æ¡ï¼‰
    recent = history[-(max_rounds * 2):]
    
    lines = []
    for msg in recent:
        role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
        content = msg["content"]
        if len(content) > 300:
            content = content[:300] + "..."
        lines.append(f"{role}: {content}")
    
    return "\nã€å¯¹è¯å†å²ï¼ˆæœ€è¿‘ {} è½®ï¼‰ã€‘\n{}\n".format(
        min(max_rounds, len(recent) // 2 + 1),
        "\n".join(lines)
    )


def _save_assistant_reply(answer_text: str):
    """å°† assistant å›å¤ä¿å­˜åˆ° chat_historyã€‚"""
    chat_history = cl.user_session.get("chat_history", [])
    # æˆªæ–­ä¿å­˜ï¼Œé¿å…å†å²è†¨èƒ€
    chat_history.append({"role": "assistant", "content": answer_text[:500]})
    cl.user_session.set("chat_history", chat_history)


# ==================== Message Handler ====================
@cl.on_message
async def main(message: cl.Message):
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
    
    import time
    start_time = time.time()
    session_id = id(cl.user_session)
    
    print(f"\n{'='*80}")
    print(f"ğŸ“¨ [ä¼šè¯ {session_id}] æ”¶åˆ°æ–°æ¶ˆæ¯")
    print(f"   æ—¶é—´: {time.strftime('%H:%M:%S')}")
    print(f"   å†…å®¹: {message.content[:50]}...")
    print(f"{'='*80}\n")
    
    # è·å–å½“å‰æ¨¡å—ï¼ˆä» chat profileï¼‰
    current_module = cl.user_session.get("chat_profile", "User Insights")
    
    # è·å–å¯¹è¯å†å²
    chat_history = cl.user_session.get("chat_history", [])
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    chat_history.append({"role": "user", "content": message.content})
    cl.user_session.set("chat_history", chat_history)
    
    # æ ¹æ®æ¨¡å—è·¯ç”±
    if current_module == "User Insights":
        await handle_user_insights(message)
    elif current_module == "Competitor Analysis":
        await handle_competitor_analysis(message)
    elif current_module == "PRD Writer":
        await handle_prd_writer(message)
    else:
        # Fallback to User Insights
        await handle_user_insights(message)


async def handle_user_insights(message: cl.Message):
    """User Insights æ¨¡å— - ç”¨æˆ·æ´å¯Ÿåˆ†æï¼ˆæ™ºèƒ½æ£€ç´¢ç‰ˆï¼‰"""
    import time
    start_time = time.time()
    
    user_query = message.content
    
    # åˆ›å»ºæ¶ˆæ¯å¹¶ç«‹å³å‘é€ï¼Œåç»­ç”¨ update
    msg = cl.Message(content="")
    await msg.send()
    
    try:
        v_retriever, g_retriever, hybrid, analyzer, llm = get_tools()
        
        # ==================== Step 0: æŸ¥è¯¢è·¯ç”±ï¼ˆåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢ï¼‰====================
        async with cl.Step(name="ğŸ¯ æŸ¥è¯¢è·¯ç”±", type="tool") as step:
            step.output = "æ­£åœ¨åˆ¤æ–­é—®é¢˜ç±»å‹..."
            
            # å…³é”®ä¿®å¤ï¼šç”¨ make_async åŒ…è£…åŒæ­¥è°ƒç”¨ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            routing_result = await cl.make_async(analyzer.needs_retrieval)(user_query, "User Insights")
            
            if not routing_result["requires_retrieval"]:
                # ä¸éœ€è¦æ£€ç´¢ï¼Œç›´æ¥è¿”å›
                step.output = f"""âœ… è·¯ç”±å®Œæˆ

**é—®é¢˜ç±»å‹**: {routing_result['query_category']}
**éœ€è¦æ£€ç´¢**: å¦
**ç†ç”±**: {routing_result['reasoning']}

ç›´æ¥è¿”å›é¢„è®¾å›å¤ï¼Œæ— éœ€æŸ¥è¯¢æ•°æ®åº“ã€‚"""
                
                msg.content = f"**{routing_result['query_category'].title()} Response**\n\n{routing_result['direct_response']}"
                await msg.update()
                return
            
            # éœ€è¦æ£€ç´¢ï¼Œç»§ç»­æµç¨‹
            step.output = f"""âœ… è·¯ç”±å®Œæˆ

**é—®é¢˜ç±»å‹**: {routing_result['query_category']}
**éœ€è¦æ£€ç´¢**: æ˜¯
**ç†ç”±**: {routing_result['reasoning']}

ç»§ç»­æ‰§è¡Œå®Œæ•´æ£€ç´¢æµç¨‹..."""
        
        # ==================== Step 1: é—®é¢˜åˆ†æ ====================
        async with cl.Step(name="ğŸ§  é—®é¢˜åˆ†æ", type="tool") as step:
            step.output = "æ­£åœ¨åˆ†æé—®é¢˜å¤æ‚åº¦å’Œæ£€ç´¢ç­–ç•¥..."
            
            analysis = await cl.make_async(analyzer.analyze_query)(user_query, "User Insights")
            
            step.output = f"""âœ… é—®é¢˜åˆ†æå®Œæˆ

**å¤æ‚åº¦**: {analysis['complexity']}
**é—®é¢˜ç±»å‹**: {analysis['query_type']}
**æ£€ç´¢ç­–ç•¥**: {', '.join(analysis['data_sources'])}
**æ£€ç´¢æ•°é‡**: {analysis['n_results']} æ¡
**åˆ†æç†ç”±**: {analysis['reasoning']}
"""
        
        # ==================== Step 2: æ™ºèƒ½åˆ†å±‚æ£€ç´¢ ====================
        async with cl.Step(name="ğŸ” æ™ºèƒ½æ··åˆæ£€ç´¢", type="retrieval") as step:
            step.output = "æ­£åœ¨æ‰§è¡Œåˆ†å±‚æ£€ç´¢ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰..."
            
            # ä½¿ç”¨ retrieve_for_user_insightsï¼ˆå†…éƒ¨å·²é›†æˆåˆ†å±‚æ£€ç´¢ï¼‰
            retrieval_result = await cl.make_async(hybrid.retrieve_for_user_insights)(user_query)
            personas = retrieval_result["personas"]
            ugc_result = retrieval_result["ugc_result"]
            ugc_docs = ugc_result["docs"]
            quality_result = ugc_result["quality_result"]
            layer = ugc_result["layer"]
            context = retrieval_result["context"]
            
            layer_names = {1: "å¿«é€Ÿ", 2: "æ ‡å‡†", 3: "æ·±åº¦"}
            step.output = f"""âœ… æ··åˆæ£€ç´¢å®Œæˆ

**çŸ¥è¯†å›¾è°±æ•°æ®**:
- ç”¨æˆ·ç”»åƒ: {len(personas)} ä¸ªï¼ˆæƒå¨æ•°æ®æºï¼‰

**å‘é‡åº“æ•°æ®**:
- æ£€ç´¢å±‚çº§: Layer {layer} ({layer_names[layer]}æ£€ç´¢)
- ç›¸å…³è¯„è®º: {len(ugc_docs)} æ¡
"""
        
        # ==================== Step 3: LLM åˆ†æï¼ˆä¼˜åŒ– Promptï¼‰====================
        async with cl.Step(name="ğŸ¤– AI æ·±åº¦åˆ†æ", type="llm") as step:
            step.output = "æ­£åœ¨è°ƒç”¨ DeepSeek R1 è¿›è¡Œæ·±åº¦åˆ†æ..."
            
            # æ„å»ºå¯¹è¯å†å²
            history_text = _build_history_text(max_rounds=3)
            
            # å¼ºåŒ–çº¦æŸçš„ Promptï¼ˆé˜²æ­¢å¹»è§‰ï¼‰
            prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç”µåŠ¨æ±½è½¦äº§å“ç»ç†ï¼Œæ“…é•¿ä»ç”¨æˆ·è¯„è®ºå’Œç”¨æˆ·ç”»åƒä¸­æå–å…³é”®æ´å¯Ÿã€‚

**é‡è¦æ ¼å¼è¦æ±‚**: è¯·å°†ä½ çš„æ€è€ƒè¿‡ç¨‹æ”¾åœ¨<think></think>æ ‡ç­¾ä¸­ï¼Œæœ€ç»ˆç­”æ¡ˆæ”¾åœ¨æ ‡ç­¾ä¹‹å¤–ã€‚

ã€ä¸¥æ ¼è¦æ±‚ - å¿…é¡»éµå®ˆã€‘
1. **ä»…åŸºäºæä¾›çš„æ•°æ®å›ç­”**: ä¸¥ç¦ä½¿ç”¨ä»»ä½•å¤–éƒ¨çŸ¥è¯†ã€å¸¸è¯†æˆ–å‡è®¾
2. **æ•°æ®ä¸è¶³æ—¶å¿…é¡»æ˜ç¡®è¯´æ˜**: å¦‚æœæä¾›çš„æ•°æ®ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºï¼Œä¸è¦ç¼–é€ 
3. **å¼ºåˆ¶å¼•ç”¨æ¥æº**: æ¯ä¸ªè®ºè¿°éƒ½å¿…é¡»å¼•ç”¨å…·ä½“æ•°æ®æ¥æºï¼ˆæ ¼å¼: [ç”»åƒX]ã€[è¯„è®ºY]ï¼‰
4. **ç¦æ­¢æ¨æµ‹**: ä¸è¦ä½¿ç”¨"å¯èƒ½"ã€"ä¸€èˆ¬æ¥è¯´"ã€"é€šå¸¸"ç­‰æ¨æµ‹æ€§è¡¨è¿°
{history_text}
ã€æ•°æ®æ¥æºã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{user_query}

ã€å›ç­”æ ¼å¼ã€‘
## æ ¸å¿ƒå‘ç°
- [åŸºäºæ•°æ®çš„å…³é”®æ´å¯Ÿï¼Œå¿…é¡»å¼•ç”¨æ¥æº]
- [å…³é”®æ´å¯Ÿ 2ï¼Œå¿…é¡»å¼•ç”¨æ¥æº]

## è¯¦ç»†åˆ†æ
[æ¯ä¸ªè®ºç‚¹éƒ½å¿…é¡»å¼•ç”¨æ•°æ®ï¼Œæ ¼å¼: [ç”»åƒX] æˆ– [è¯„è®ºY] å…·ä½“å†…å®¹]

## æ•°æ®è¦†ç›–åº¦è¯´æ˜
- èƒ½å¤Ÿå›ç­”çš„æ–¹é¢: [åˆ—å‡ºå“ªäº›æ–¹é¢æœ‰æ•°æ®æ”¯æŒ]
- æ•°æ®ä¸è¶³çš„æ–¹é¢: [åˆ—å‡ºå“ªäº›æ–¹é¢æ•°æ®ä¸è¶³ï¼Œæ— æ³•å›ç­”]

è¯·ä¸¥æ ¼åŸºäºä¸Šè¿°æ•°æ®å›ç­”ï¼Œä¸è¦æ·»åŠ ä»»ä½•æ¨æµ‹æˆ–å¤–éƒ¨çŸ¥è¯†ã€‚"""
            
            print(f"\n{'='*60}")
            print(f"ğŸ” å¼€å§‹ LLM åˆ†æ")
            print(f"Context é•¿åº¦: {len(context)} å­—ç¬¦")
            print(f"{'='*60}\n")
            
            response = await cl.make_async(llm.chat)(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=2000
            )
            
            print(f"\n{'='*60}")
            print(f"âœ… LLM å“åº”æˆåŠŸ")
            print(f"Response type: {type(response)}")
            if hasattr(response, 'choices') and len(response.choices) > 0:
                print(f"Content length: {len(response.choices[0].message.content)}")
                print(f"Content preview: {response.choices[0].message.content[:200]}...")
            print(f"{'='*60}\n")
            
            raw_output = response.choices[0].message.content
            print(f"\nâœ… è·å–åŸå§‹è¾“å‡ºï¼Œé•¿åº¦: {len(raw_output)}")
            step.output = f"âœ… åˆ†æå®Œæˆï¼ˆToken: ~{response.usage.total_tokens if hasattr(response, 'usage') else 'N/A'}ï¼‰"
        
        # æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡º - ç®€åŒ–çš„å¥å£®å¤„ç†
        print(f"\n{'='*60}")
        print(f"ğŸ“ å¼€å§‹æ ¼å¼åŒ–è¾“å‡º")
        print(f"Raw output é•¿åº¦: {len(raw_output)}")
        print(f"{'='*60}\n")
        
        try:
            # å°è¯•æå–æ€è€ƒé“¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œä½†ä¸å¼ºåˆ¶è¦æ±‚
            thought_chain = None
            final_answer = raw_output  # é»˜è®¤ä½¿ç”¨å…¨éƒ¨å†…å®¹
            
            # å¦‚æœåŒ…å« </think> æ ‡ç­¾ï¼Œå°è¯•åˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
            if "</think>" in raw_output:
                # æŸ¥æ‰¾ç»“æŸæ ‡ç­¾ä½ç½®
                think_end = raw_output.find("</think>")
                
                # å°è¯•æŸ¥æ‰¾å¼€å§‹æ ‡ç­¾
                think_start = 0
                if "<think>" in raw_output:
                    think_start = raw_output.find("<think>") + 7  # è·³è¿‡ <think>
                
                potential_thought = raw_output[think_start:think_end].strip()
                potential_answer = raw_output[think_end + 8:].strip()  # è·³è¿‡ </think>
                
                # éªŒè¯æå–ç»“æœ
                if potential_answer:  # åªè¦æœ‰ç­”æ¡ˆå°±å¯ä»¥
                    if potential_thought:
                        thought_chain = potential_thought
                    final_answer = potential_answer
                    print(f"âœ… æˆåŠŸåˆ†ç¦»å†…å®¹ - æ€è€ƒé“¾: {len(thought_chain or '')} å­—ç¬¦, ç­”æ¡ˆ: {len(final_answer)} å­—ç¬¦")
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°ç­”æ¡ˆéƒ¨åˆ†ï¼Œä½¿ç”¨å…¨éƒ¨å†…å®¹")
                    final_answer = raw_output
            else:
                print(f"âš ï¸ æœªæ£€æµ‹åˆ° </think> æ ‡ç­¾ï¼Œç›´æ¥ä½¿ç”¨å…¨éƒ¨å†…å®¹")
            
            # æ„å»ºæœ€ç»ˆæ¶ˆæ¯ï¼ˆç¡®ä¿ä¸€å®šæœ‰å†…å®¹ï¼‰
            
            print(f"\nğŸ“ å‡†å¤‡å‘é€å›å¤:")
            print(f"   ç­”æ¡ˆé•¿åº¦: {len(final_answer)} å­—ç¬¦")
            print(f"   ç­”æ¡ˆé¢„è§ˆ: {final_answer[:100]}...")
            
            # æ„å»ºå¯æŠ˜å çš„æ•°æ®æº¯æºåŒºåŸŸ
            source_sections = []
            
            # ç”¨æˆ·ç”»åƒæº¯æº
            if personas:
                persona_lines = []
                for i, p in enumerate(personas, 1):
                    dims = ', '.join([d['dimension'] for d in p.get('top_dimensions', [])])
                    persona_lines.append(
                        f"**[ç”»åƒ {i}] {p.get('persona_name', 'Unknown')}** â€” "
                        f"è¯„è®ºæ•°: {p.get('review_count', 0)} | ä¼˜å…ˆç»´åº¦: {dims}"
                    )
                source_sections.append(
                    f"<details><summary>ğŸ“Š çŸ¥è¯†å›¾è°± Â· ç”¨æˆ·ç”»åƒï¼ˆ{len(personas)} ä¸ªï¼‰</summary>\n\n"
                    + "\n\n".join(persona_lines)
                    + "\n\n</details>"
                )
            
            # è¯„è®ºæº¯æº
            if ugc_docs:
                review_lines = []
                for i, doc in enumerate(ugc_docs[:15], 1):
                    meta = doc.get('metadata', {})
                    brand = meta.get('brand', '')
                    series = meta.get('series', '')
                    model = meta.get('model', '')
                    dim = meta.get('dimension', '')
                    text = doc.get('text', '')
                    # æˆªæ–­è¿‡é•¿çš„è¯„è®º
                    if len(text) > 200:
                        text = text[:200] + "..."
                    review_lines.append(
                        f"**[è¯„è®º {i}]** {brand} {series} {model} Â· {dim}\n> {text}"
                    )
                source_sections.append(
                    f"<details><summary>ğŸ’¬ å‘é‡æ£€ç´¢ Â· ç”¨æˆ·è¯„è®ºï¼ˆ{len(ugc_docs)} æ¡ï¼‰</summary>\n\n"
                    + "\n\n".join(review_lines)
                    + "\n\n</details>"
                )
            
            source_block = "\n\n".join(source_sections)
            
            msg.content = f"""**ğŸ“Š ç”¨æˆ·æ´å¯Ÿåˆ†æ**

{final_answer}

---

{source_block}

<sub>æ•°æ®æ¥æº: çŸ¥è¯†å›¾è°± + å‘é‡åº“ | ç”»åƒ: {len(personas)} ä¸ª Â· è¯„è®º: {len(ugc_docs)} æ¡</sub>"""
            
            # å¦‚æœæˆåŠŸæå–æ€è€ƒé“¾ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯çš„ elements ä¸­ï¼ˆä¸åˆ›å»ºæ–°æ¶ˆæ¯ï¼‰
            if thought_chain:
                msg.elements = [
                    cl.Text(name="ğŸ’­ æ€è€ƒè¿‡ç¨‹", content=thought_chain, display="side")
                ]
            
            print(f"âœ… æ¶ˆæ¯æ ¼å¼åŒ–æˆåŠŸï¼Œæœ€ç»ˆå†…å®¹é•¿åº¦: {len(msg.content)}")
            
            # ä¸€æ¬¡æ€§å‘é€å®Œæ•´æ¶ˆæ¯ï¼ˆè€Œä¸æ˜¯å…ˆ send å† updateï¼‰
            await msg.send()
            
            # ä¿å­˜ assistant å›å¤åˆ°å¯¹è¯å†å²
            _save_assistant_reply(final_answer)
            
            # è®°å½•å‘é€åçŠ¶æ€
            elapsed = time.time() - start_time
            print(f"âœ… æ¶ˆæ¯å·²å‘é€")
            print(f"â±ï¸  æ€»è€—æ—¶: {elapsed:.1f}ç§’")
            print(f"\n{'='*80}\n")
            
        except Exception as e:
            # æ•è·æ ¼å¼åŒ–/å‘é€å¼‚å¸¸
            print(f"\nâŒ æ ¼å¼åŒ–æˆ–å‘é€å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # ä½¿ç”¨åŸå§‹å†…å®¹
            try:
                fallback_msg = f"**ğŸ“Š ç”¨æˆ·æ´å¯Ÿåˆ†æ**\n\n{raw_output}\n\n---\n*æ•°æ®æ¥æº: çŸ¥è¯†å›¾è°± + å‘é‡åº“*"
                msg.content = fallback_msg
                await msg.send()
            except:
                pass
        
    except Exception as e:
        # æœ€å¤–å±‚å¼‚å¸¸å¤„ç†
        print(f"âŒ åˆ†æå‡ºé”™: {e}")
        try:
            msg.content = f"âŒ åˆ†æå‡ºé”™: {str(e)}"
            await msg.send()
        except:
            pass


async def handle_competitor_analysis(message: cl.Message):
    """Competitor Analysis æ¨¡å— - ç«å“åˆ†æï¼ˆæ™ºèƒ½æ£€ç´¢ç‰ˆï¼‰"""
    user_query = message.content
    
    msg = cl.Message(content="")
    await msg.send()
    
    try:
        v_retriever, g_retriever, hybrid, analyzer, llm = get_tools()
        
        # ==================== Step 0: æŸ¥è¯¢è·¯ç”± ====================
        async with cl.Step(name="ğŸ¯ æŸ¥è¯¢è·¯ç”±", type="tool") as step:
            step.output = "æ­£åœ¨åˆ¤æ–­é—®é¢˜ç±»å‹..."
            
            routing_result = await cl.make_async(analyzer.needs_retrieval)(user_query, "Competitor Analysis")
            
            if not routing_result["requires_retrieval"]:
                step.output = f"""âœ… è·¯ç”±å®Œæˆ

**é—®é¢˜ç±»å‹**: {routing_result['query_category']}
**éœ€è¦æ£€ç´¢**: å¦
**ç†ç”±**: {routing_result['reasoning']}

ç›´æ¥è¿”å›é¢„è®¾å›å¤ã€‚"""
                
                msg.content = f"**{routing_result['query_category'].title()} Response**\n\n{routing_result['direct_response']}"
                await msg.update()
                return
            
            step.output = f"""âœ… è·¯ç”±å®Œæˆ

**é—®é¢˜ç±»å‹**: {routing_result['query_category']}
**éœ€è¦æ£€ç´¢**: æ˜¯
**ç†ç”±**: {routing_result['reasoning']}

ç»§ç»­æ‰§è¡Œç«å“åˆ†ææµç¨‹..."""
        
        # ==================== Step 1: é—®é¢˜åˆ†æ ====================
        async with cl.Step(name="ğŸ§  é—®é¢˜åˆ†æ", type="tool") as step:
            step.output = "æ­£åœ¨åˆ†æç«å“åˆ†æéœ€æ±‚..."
            
            analysis = await cl.make_async(analyzer.analyze_query)(user_query, "Competitor Analysis")
            
            step.output = f"""âœ… åˆ†æå®Œæˆ

**å¤æ‚åº¦**: {analysis['complexity']}
**é—®é¢˜ç±»å‹**: {analysis['query_type']}
**æ£€ç´¢ç­–ç•¥**: {', '.join(analysis['data_sources'])}
**æ£€ç´¢æ•°é‡**: {analysis['n_results']} æ¡
"""
        
        # ==================== Step 1.5: å®ä½“æå– ====================
        async with cl.Step(name="ğŸ·ï¸ å®ä½“æå–", type="tool") as step:
            step.output = "æ­£åœ¨ä»é—®é¢˜ä¸­æå–å“ç‰Œå’Œè½¦å‹..."
            
            entities = await cl.make_async(analyzer.extract_entities)(user_query, "Competitor Analysis")
            extracted_brands = entities["brands"]
            extracted_models = entities["models"]
            
            step.output = f"""âœ… å®ä½“æå–å®Œæˆ

**æå–çš„å“ç‰Œ**: {', '.join(extracted_brands) if extracted_brands else 'æ— '}
**æå–çš„è½¦å‹**: {', '.join(extracted_models) if extracted_models else 'æ— '}
**ç½®ä¿¡åº¦**: {entities['extraction_confidence']:.0%}
"""
        
        # ==================== Step 2: æ™ºèƒ½åˆ†å±‚æ£€ç´¢ ====================
        async with cl.Step(name="ğŸ” æ™ºèƒ½æ··åˆæ£€ç´¢", type="retrieval") as step:
            step.output = "æ­£åœ¨æ‰§è¡Œåˆ†å±‚æ£€ç´¢ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰..."
            
            # ä½¿ç”¨ retrieve_for_competitor_analysisï¼ˆå†…éƒ¨å·²é›†æˆåˆ†å±‚æ£€ç´¢ï¼‰
            retrieval_result = await cl.make_async(hybrid.retrieve_for_competitor_analysis)(user_query, brands=extracted_brands)
            vehicles = retrieval_result["vehicles"]
            ugc_result = retrieval_result["ugc_result"]
            ugc_docs = ugc_result["docs"]
            quality_result = ugc_result["quality_result"]
            layer = ugc_result["layer"]
            spec_docs = retrieval_result["spec_docs"]
            context = retrieval_result["context"]
            
            layer_names = {1: "å¿«é€Ÿ", 2: "æ ‡å‡†", 3: "æ·±åº¦"}
            step.output = f"""âœ… æ··åˆæ£€ç´¢å®Œæˆ

**çŸ¥è¯†å›¾è°±æ•°æ®**:
- è½¦å‹å‚æ•°: {len(vehicles)} ä¸ª

**å‘é‡åº“æ•°æ®**:
- æ£€ç´¢å±‚çº§: Layer {layer} ({layer_names[layer]}æ£€ç´¢)
- ç›¸å…³è¯„è®º: {len(ugc_docs)} æ¡
- è§„æ ¼æ–‡æ¡£: {len(spec_docs)} ä¸ª
"""
        
        # ==================== Step 3: LLM ç«å“åˆ†æ ====================
        async with cl.Step(name="âš”ï¸ ç«å“å¯¹æ¯”åˆ†æ", type="llm") as step:
            step.output = "æ­£åœ¨ç”Ÿæˆç«å“åˆ†ææŠ¥å‘Š..."
            
            # æ„å»ºå¯¹è¯å†å²
            history_text = _build_history_text(max_rounds=3)
            
            prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç”µåŠ¨æ±½è½¦äº§å“åˆ†æå¸ˆï¼Œæ“…é•¿ç«å“å¯¹æ¯”å’Œå¸‚åœºæ´å¯Ÿã€‚

ã€ä¸¥æ ¼è¦æ±‚ - å¿…é¡»éµå®ˆã€‘
1. **ä»…åŸºäºæä¾›çš„æ•°æ®**: ä¸¥ç¦ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†æˆ–å¸‚åœºå¸¸è¯†
2. **æ•°æ®ä¸è¶³æ—¶æ˜ç¡®è¯´æ˜**: å¦‚æœæŸæ–¹é¢æ•°æ®ç¼ºå¤±ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºè€Œéæ¨æµ‹
3. **å¼ºåˆ¶å¼•ç”¨æ¥æº**: æ¯ä¸ªå‚æ•°ã€è¯„ä»·éƒ½å¿…é¡»æ ‡æ³¨æ¥æº
4. **é¿å…ä¸»è§‚æè¿°**: ä½¿ç”¨å®¢è§‚çš„æ•°æ®å¯¹æ¯”ï¼Œä¸è¦ä½¿ç”¨"æ›´å¥½"ã€"ä¼˜ç§€"ç­‰ä¸»è§‚è¯æ±‡ï¼ˆé™¤éæœ‰æ•°æ®æ”¯æŒï¼‰
{history_text}
ã€æ•°æ®æ¥æºã€‘
{context}

ã€ç”¨æˆ·éœ€æ±‚ã€‘
{user_query}

ã€åˆ†ææ¡†æ¶ã€‘
## 1. æ ¸å¿ƒå‚æ•°å¯¹æ¯”
[ä»…åˆ—å‡ºæ•°æ®ä¸­æœ‰çš„å‚æ•°ï¼Œæ ‡æ³¨æ¥æº]

## 2. ç”¨æˆ·è¯„ä»·å¯¹æ¯”
[åŸºäºçœŸå®è¯„è®ºï¼Œå¼•ç”¨æ ¼å¼: [è¯„è®ºX] å…·ä½“å†…å®¹]

## 3. ä¼˜åŠ£åŠ¿åˆ†æ
- ä¼˜åŠ¿: [åŸºäºæ•°æ®çš„å®¢è§‚å¯¹æ¯”]
- åŠ£åŠ¿: [åŸºäºæ•°æ®çš„å®¢è§‚å¯¹æ¯”]

## 4. æ•°æ®è¦†ç›–è¯´æ˜
- æ•°æ®å®Œæ•´çš„æ–¹é¢: [åˆ—å‡º]
- æ•°æ®ç¼ºå¤±çš„æ–¹é¢: [æ˜ç¡®æŒ‡å‡ºæ— æ³•å¯¹æ¯”çš„éƒ¨åˆ†]

è¯·ä¸¥æ ¼åŸºäºä¸Šè¿°æ•°æ®ç”ŸæˆæŠ¥å‘Šï¼Œä¸è¦æ·»åŠ æ¨æµ‹ã€‚"""
            
            response = await cl.make_async(llm.chat)(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,  # é™ä½æ¸©åº¦æé«˜ç¡®å®šæ€§
                max_tokens=2000
            )
            
            raw_output = response.choices[0].message.content
            step.output = "âœ… åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ"
        
        # ==================== æ ¼å¼åŒ–è¾“å‡ºï¼ˆç®€åŒ–å¥å£®å¤„ç†ï¼‰====================
        try:
            # å°è¯•æå–æ€è€ƒé“¾
            thought_chain = None
            final_answer = raw_output
            
            if "</think>" in raw_output:
                think_end = raw_output.find("</think>")
                potential_thought = raw_output[:think_end].strip()
                potential_answer = raw_output[think_end + 8:].strip()
                
                if potential_thought and potential_answer:
                    thought_chain = potential_thought
                    final_answer = potential_answer
            
            # å±•ç¤ºæ€è€ƒé“¾ï¼ˆå¦‚æœ‰ï¼‰- é™„åŠ åˆ°æ¶ˆæ¯ elements ä¸Šï¼Œä¸åˆ›å»ºæ–°æ¶ˆæ¯
            if thought_chain:
                msg.elements = [cl.Text(name="ğŸ’­ åˆ†ææ€è·¯", content=thought_chain, display="side")]
            
            # æ„å»ºæœ€ç»ˆæ¶ˆæ¯
            data_note = f"{len(vehicles)} ä¸ªè½¦å‹å‚æ•° + " if vehicles else ""
            
            # æ„å»ºå¯æŠ˜å çš„æ•°æ®æº¯æºåŒºåŸŸ
            source_sections = []
            
            # è½¦å‹å‚æ•°æº¯æº
            if vehicles:
                vehicle_lines = []
                for v in vehicles:
                    vehicle_lines.append(
                        f"**{v.get('brand', '')} {v.get('series', '')} {v.get('model', '')}** â€” "
                        f"ä»·æ ¼: {v.get('price', 'N/A')}ä¸‡ | ç»­èˆª: {v.get('range_cltc', 'N/A')}km | "
                        f"åŠ é€Ÿ: {v.get('acceleration', 'N/A')}s | åº§ä½: {v.get('seats', 'N/A')}"
                    )
                source_sections.append(
                    f"<details><summary>ğŸš— çŸ¥è¯†å›¾è°± Â· è½¦å‹å‚æ•°ï¼ˆ{len(vehicles)} ä¸ªï¼‰</summary>\n\n"
                    + "\n\n".join(vehicle_lines)
                    + "\n\n</details>"
                )
            
            # è¯„è®ºæº¯æº
            if ugc_docs:
                review_lines = []
                for i, doc in enumerate(ugc_docs[:15], 1):
                    meta = doc.get('metadata', {})
                    brand = meta.get('brand', '')
                    series = meta.get('series', '')
                    model = meta.get('model', '')
                    dim = meta.get('dimension', '')
                    text = doc.get('text', '')
                    if len(text) > 200:
                        text = text[:200] + "..."
                    review_lines.append(
                        f"**[è¯„è®º {i}]** {brand} {series} {model} Â· {dim}\n> {text}"
                    )
                source_sections.append(
                    f"<details><summary>ğŸ’¬ å‘é‡æ£€ç´¢ Â· ç”¨æˆ·è¯„è®ºï¼ˆ{len(ugc_docs)} æ¡ï¼‰</summary>\n\n"
                    + "\n\n".join(review_lines)
                    + "\n\n</details>"
                )
            
            # è§„æ ¼æ–‡æ¡£æº¯æº
            if spec_docs:
                spec_lines = []
                for i, doc in enumerate(spec_docs[:10], 1):
                    meta = doc.get('metadata', {})
                    text = doc.get('text', '')
                    if len(text) > 200:
                        text = text[:200] + "..."
                    spec_lines.append(
                        f"**[è§„æ ¼ {i}]** {meta.get('brand', '')} {meta.get('series', '')} {meta.get('model', '')}\n> {text}"
                    )
                source_sections.append(
                    f"<details><summary>ğŸ“‹ å‘é‡æ£€ç´¢ Â· è½¦å‹è§„æ ¼ï¼ˆ{len(spec_docs)} ä¸ªï¼‰</summary>\n\n"
                    + "\n\n".join(spec_lines)
                    + "\n\n</details>"
                )
            
            source_block = "\n\n".join(source_sections)
            
            msg.content = f"""**âš”ï¸ ç«å“åˆ†ææŠ¥å‘Š**

{final_answer}

---

{source_block}

<sub>æ•°æ®æ¥æº: çŸ¥è¯†å›¾è°± + å‘é‡åº“ | è½¦å‹: {len(vehicles)} ä¸ª Â· è¯„è®º: {len(ugc_docs)} æ¡ Â· è§„æ ¼: {len(spec_docs)} ä¸ª</sub>"""
            
            await msg.update()
            
            # ä¿å­˜ assistant å›å¤åˆ°å¯¹è¯å†å²
            _save_assistant_reply(final_answer)
        except Exception as e:
            # é™çº§æ˜¾ç¤º
            data_note = f"{len(vehicles)} ä¸ªè½¦å‹å‚æ•° + " if vehicles else ""
            msg.content = f"""**âš”ï¸ ç«å“åˆ†ææŠ¥å‘Š**

{raw_output}

---
*åŸºäº {data_note}{len(ugc_docs)} æ¡è¯„è®ºå’Œ {len(spec_docs)} ä¸ªè½¦å‹æ•°æ®*"""
            await msg.update()
        
    except Exception as e:
        msg.content = f"âŒ åˆ†æå‡ºé”™: {str(e)}"
        await msg.update()


async def handle_prd_writer(message: cl.Message):
    """PRD Writer æ¨¡å— - PRD æ–‡æ¡£æ’°å†™ï¼ˆæ™ºèƒ½æ£€ç´¢ç‰ˆï¼‰"""
    user_query = message.content
    
    msg = cl.Message(content="")
    await msg.send()
    
    try:
        v_retriever, g_retriever, hybrid, analyzer, llm = get_tools()
        
        # ==================== Step 0: æŸ¥è¯¢è·¯ç”± ====================
        async with cl.Step(name="ğŸ¯ æŸ¥è¯¢è·¯ç”±", type="tool") as step:
            step.output = "æ­£åœ¨åˆ¤æ–­é—®é¢˜ç±»å‹..."
            
            routing_result = await cl.make_async(analyzer.needs_retrieval)(user_query, "User Insights")
            
            if not routing_result["requires_retrieval"]:
                step.output = f"""âœ… è·¯ç”±å®Œæˆ

**é—®é¢˜ç±»å‹**: {routing_result['query_category']}
**éœ€è¦æ£€ç´¢**: å¦

ç›´æ¥è¿”å›é¢„è®¾å›å¤ã€‚"""
                
                msg.content = f"**{routing_result['query_category'].title()} Response**\n\n{routing_result['direct_response']}"
                await msg.update()
                return
            
            step.output = f"""âœ… è·¯ç”±å®Œæˆ

**é—®é¢˜ç±»å‹**: {routing_result['query_category']}
**éœ€è¦æ£€ç´¢**: æ˜¯

ç»§ç»­æ‰§è¡Œ PRD æ’°å†™æµç¨‹..."""
        
        # ==================== Step 1: é—®é¢˜åˆ†æ ====================
        async with cl.Step(name="ğŸ§  é—®é¢˜åˆ†æ", type="tool") as step:
            step.output = "æ­£åœ¨åˆ†æ PRD æ’°å†™éœ€æ±‚..."
            
            analysis = await cl.make_async(analyzer.analyze_query)(user_query, "PRD Writer")
            
            step.output = f"""âœ… åˆ†æå®Œæˆ

**å¤æ‚åº¦**: {analysis['complexity']}
**é—®é¢˜ç±»å‹**: {analysis['query_type']}
**æ£€ç´¢æ•°é‡**: {analysis['n_results']} æ¡ï¼ˆPRD éœ€è¦æ›´å¤šæ•°æ®ï¼‰
"""
        
        # ==================== Step 1.5: å®ä½“æå– ====================
        async with cl.Step(name="ğŸ·ï¸ å®ä½“æå–", type="tool") as step:
            step.output = "æ­£åœ¨ä»é—®é¢˜ä¸­æå–å“ç‰Œå’Œè½¦å‹..."
            
            entities = await cl.make_async(analyzer.extract_entities)(user_query, "PRD Writer")
            extracted_brands = entities["brands"]
            extracted_models = entities["models"]
            
            step.output = f"""âœ… å®ä½“æå–å®Œæˆ

**æå–çš„å“ç‰Œ**: {', '.join(extracted_brands) if extracted_brands else 'æ— '}
**æå–çš„è½¦å‹**: {', '.join(extracted_models) if extracted_models else 'æ— '}
**ç½®ä¿¡åº¦**: {entities['extraction_confidence']:.0%}
"""
        
        # ==================== Step 2: æ™ºèƒ½åˆ†å±‚æ£€ç´¢ ====================
        async with cl.Step(name="ğŸ” å…¨æ ˆæ•°æ®æ£€ç´¢", type="retrieval") as step:
            step.output = "æ­£åœ¨æ‰§è¡Œåˆ†å±‚æ£€ç´¢ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰..."
            
            # ä½¿ç”¨ retrieve_for_prdï¼ˆå†…éƒ¨å·²é›†æˆåˆ†å±‚æ£€ç´¢ï¼‰
            retrieval_result = await cl.make_async(hybrid.retrieve_for_prd)(user_query, brands=extracted_brands)
            personas = retrieval_result["personas"]
            vehicles = retrieval_result["vehicles"]
            ugc_result = retrieval_result["ugc_result"]
            ugc_docs = ugc_result["docs"]
            quality_result = ugc_result["quality_result"]
            layer = ugc_result["layer"]
            spec_docs = retrieval_result["spec_docs"]
            context = retrieval_result["context"]
            
            layer_names = {1: "å¿«é€Ÿ", 2: "æ ‡å‡†", 3: "æ·±åº¦"}
            step.output = f"""âœ… å…¨æ ˆæ£€ç´¢å®Œæˆ

**çŸ¥è¯†å›¾è°±æ•°æ®**:
- ç”¨æˆ·ç”»åƒ: {len(personas)} ä¸ª
- è½¦å‹å‚æ•°: {len(vehicles)} ä¸ª

**å‘é‡åº“æ•°æ®**:
- æ£€ç´¢å±‚çº§: Layer {layer} ({layer_names[layer]}æ£€ç´¢)
- ç”¨æˆ·è¯„è®º: {len(ugc_docs)} æ¡
- è§„æ ¼æ–‡æ¡£: {len(spec_docs)} ä¸ª
"""
        
        # ==================== Step 5: PRD ç”Ÿæˆ ====================
        async with cl.Step(name="ğŸ“ ç”Ÿæˆ PRD æ–‡æ¡£", type="llm") as step:
            step.output = "æ­£åœ¨ä½¿ç”¨ DeepSeek R1 ç”Ÿæˆäº§å“éœ€æ±‚æ–‡æ¡£..."
            
            # æ„å»ºå¯¹è¯å†å²
            history_text = _build_history_text(max_rounds=3)
            
            prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç”µåŠ¨æ±½è½¦äº§å“ç»ç†ï¼Œæ“…é•¿æ’°å†™ä¸“ä¸šçš„äº§å“éœ€æ±‚æ–‡æ¡£ï¼ˆPRDï¼‰ã€‚

ã€ä¸¥æ ¼è¦æ±‚ - é˜²æ­¢å¹»è§‰ã€‘
1. **ä»…åŸºäºæä¾›æ•°æ®**: ä¸¥ç¦ä½¿ç”¨è¡Œä¸šå¸¸è¯†ã€é¢„è®­ç»ƒçŸ¥è¯†æˆ–å‡è®¾
2. **æ•°æ®ä¸è¶³æ—¶åŠ¡å¿…è¯´æ˜**: å¦‚æœæŸä¸ªPRDç« èŠ‚ç¼ºä¹æ•°æ®æ”¯æŒï¼Œè¯·æ˜ç¡®è¯´æ˜
3. **æ‰€æœ‰éœ€æ±‚å¿…é¡»æº¯æº**: æ¯ä¸ªåŠŸèƒ½éœ€æ±‚éƒ½å¿…é¡»å¼•ç”¨å…·ä½“æ•°æ®æ¥æº
4. **ç¦æ­¢è‡†é€ ç”¨æˆ·ç”»åƒ**: åªä½¿ç”¨æä¾›çš„æƒå¨ç”»åƒæ•°æ®ï¼Œä¸è¦åˆ›é€ æ–°çš„ç”¨æˆ·ç±»å‹
5. **ç¦æ­¢è‡†é€ ç«å“ä¿¡æ¯**: åªå¯¹æ¯”æä¾›çš„è½¦å‹æ•°æ®
{history_text}
ã€æ•°æ®æ¥æºã€‘
{context}

ã€ç”¨æˆ·éœ€æ±‚ã€‘
{user_query}

ã€PRD ç»“æ„è¦æ±‚ã€‘

## 1. é¡¹ç›®èƒŒæ™¯
[åŸºäºæä¾›çš„è¯„è®ºå’Œç”»åƒæ•°æ®åˆ†æå¸‚åœºæœºä¼šï¼Œæ ‡æ³¨æ¥æº]

## 2. ç›®æ ‡ç”¨æˆ·
[ä»…ä½¿ç”¨æä¾›çš„æƒå¨ç”¨æˆ·ç”»åƒï¼Œå¼•ç”¨æ ¼å¼: [ç”»åƒX]]

## 3. å¸‚åœºåˆ†æ
[åŸºäºæä¾›çš„è¯„è®ºå’Œè½¦å‹æ•°æ®ï¼Œå¼•ç”¨å…·ä½“æ¥æº]

## 4. æ ¸å¿ƒåŠŸèƒ½éœ€æ±‚
[æ¯ä¸ªéœ€æ±‚å¿…é¡»æ ‡æ³¨æ¥æºï¼Œæ ¼å¼: éœ€æ±‚X - æ¥æº: [è¯„è®ºY]]
- P0: [æ ¸å¿ƒéœ€æ±‚ï¼Œå¼•ç”¨æ•°æ®]
- P1: [é‡è¦éœ€æ±‚ï¼Œå¼•ç”¨æ•°æ®]
- P2: [å¯é€‰éœ€æ±‚ï¼Œå¼•ç”¨æ•°æ®]

## 5. æ•°æ®è¦†ç›–åº¦è¯´æ˜
- æ•°æ®å®Œæ•´çš„éƒ¨åˆ†: [å“ªäº›éœ€æ±‚æœ‰å……åˆ†æ•°æ®æ”¯æŒ]
- éœ€è¦è¡¥å……è°ƒç ”çš„éƒ¨åˆ†: [å“ªäº›æ–¹é¢æ•°æ®ä¸è¶³ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶]

## 6. æˆåŠŸæŒ‡æ ‡
[åŸºäºæ•°æ®ä¸­çš„ç”¨æˆ·æœŸæœ›å’Œç—›ç‚¹ï¼Œè®¾å®šå¯é‡åŒ–æŒ‡æ ‡]

ã€é‡è¦ã€‘æ‰€æœ‰å†…å®¹å¿…é¡»åŸºäºæä¾›çš„æ•°æ®ï¼Œä¸è¦æ·»åŠ ä»»ä½•æ¨æµ‹ã€‚å¦‚æœæ•°æ®ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜å“ªäº›éƒ¨åˆ†éœ€è¦è¡¥å……è°ƒç ”ã€‚"""
            
            response = await cl.make_async(llm.chat)(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,  # é™ä½æ¸©åº¦æé«˜ç¡®å®šæ€§
                max_tokens=3000
            )
            
            raw_output = response.choices[0].message.content
            step.output = f"âœ… PRD ç”Ÿæˆå®Œæˆï¼ˆToken: ~{response.usage.total_tokens if hasattr(response, 'usage') else 'N/A'}ï¼‰"
        
        # ==================== è§£ææ€è€ƒé“¾ ====================
        if "<think>" in raw_output and "</think>" in raw_output:
            think_start = raw_output.find("<think>") + 7
            think_end = raw_output.find("</think>")
            thought_chain = raw_output[think_start:think_end].strip()
            final_prd = raw_output[think_end + 8:].strip()
            
            msg.elements = [
                cl.Text(name="ğŸ’­ PRD æ’°å†™æ€è·¯ï¼ˆå¯å±•å¼€ï¼‰", content=thought_chain, display="side")
            ]
            
            prd_content = final_prd
        else:
            prd_content = raw_output
        
        # ==================== æ˜¾ç¤º PRD ====================
        # æ„å»ºå¯æŠ˜å çš„æ•°æ®æº¯æºåŒºåŸŸ
        source_sections = []
        
        # ç”¨æˆ·ç”»åƒæº¯æº
        if personas:
            persona_lines = []
            for i, p in enumerate(personas, 1):
                dims = ', '.join([d['dimension'] for d in p.get('top_dimensions', [])])
                persona_lines.append(
                    f"**[ç”»åƒ {i}] {p.get('persona_name', 'Unknown')}** â€” "
                    f"è¯„è®ºæ•°: {p.get('review_count', 0)} | ä¼˜å…ˆç»´åº¦: {dims}"
                )
            source_sections.append(
                f"<details><summary>ğŸ“Š çŸ¥è¯†å›¾è°± Â· ç”¨æˆ·ç”»åƒï¼ˆ{len(personas)} ä¸ªï¼‰</summary>\n\n"
                + "\n\n".join(persona_lines)
                + "\n\n</details>"
            )
        
        # è¯„è®ºæº¯æº
        if ugc_docs:
            review_lines = []
            for i, doc in enumerate(ugc_docs[:20], 1):
                meta = doc.get('metadata', {})
                brand = meta.get('brand', '')
                series = meta.get('series', '')
                model = meta.get('model', '')
                dim = meta.get('dimension', '')
                text = doc.get('text', '')
                if len(text) > 200:
                    text = text[:200] + "..."
                review_lines.append(
                    f"**[è¯„è®º {i}]** {brand} {series} {model} Â· {dim}\n> {text}"
                )
            source_sections.append(
                f"<details><summary>ğŸ’¬ å‘é‡æ£€ç´¢ Â· ç”¨æˆ·è¯„è®ºï¼ˆ{len(ugc_docs)} æ¡ï¼‰</summary>\n\n"
                + "\n\n".join(review_lines)
                + "\n\n</details>"
            )
        
        source_block = "\n\n".join(source_sections)
        
        msg.content = f"""**ğŸ“ äº§å“éœ€æ±‚æ–‡æ¡£ï¼ˆPRDï¼‰**

{prd_content}

---

{source_block}

<sub>æ•°æ®æ¥æº: çŸ¥è¯†å›¾è°± + å‘é‡åº“ | ç”»åƒ: {len(personas)} ä¸ª Â· è½¦å‹: {len(vehicles)} ä¸ª Â· è¯„è®º: {len(ugc_docs)} æ¡</sub>

ğŸ’¡ **æç¤º**: æ‚¨å¯ä»¥å¤åˆ¶ä¸Šè¿°å†…å®¹ä¿å­˜ä¸º Markdown æ–‡ä»¶ï¼Œæˆ–ç›´æ¥ç”¨äºäº§å“è§„åˆ’ã€‚
"""
        
        await msg.update()
        
        # ä¿å­˜ assistant å›å¤åˆ°å¯¹è¯å†å²
        _save_assistant_reply(prd_content)
        
        # ==================== å¯é€‰ï¼šæä¾›å¯¼å‡ºåŠŸèƒ½ ====================
        actions = [
            cl.Action(name="export_prd", payload={"content": prd_content[:3000]}, label="ğŸ“¥ å¯¼å‡ºä¸º Markdown æ–‡ä»¶")
        ]
        await cl.Message(content="", actions=actions).send()
        
    except Exception as e:
        msg.content = f"âŒ PRD ç”Ÿæˆå‡ºé”™: {str(e)}"
        await msg.update()


@cl.action_callback("export_prd")
async def on_export_prd(action: cl.Action):
    """å¤„ç† PRD å¯¼å‡º"""
    prd_content = action.payload.get("content", "")
    
    # åˆ›å»ºæ–‡æœ¬å…ƒç´ ä¾›ä¸‹è½½
    elements = [
        cl.Text(name="PRD.md", content=prd_content, display="inline")
    ]
    
    await cl.Message(
        content="âœ… PRD æ–‡æ¡£å·²å‡†å¤‡å¥½ï¼Œæ‚¨å¯ä»¥å¤åˆ¶ä¸Šæ–¹å†…å®¹ã€‚",
        elements=elements
    ).send()


if __name__ == "__main__":
    # Chainlit will handle the server startup
    pass
