"""
Query Analyzer - é—®é¢˜åˆ†æžæ¨¡å—
åˆ¤æ–­ç”¨æˆ·é—®é¢˜çš„å¤æ‚åº¦ã€ç±»åž‹å’Œæ‰€éœ€æ£€ç´¢ç­–ç•¥

Author: EV PM DSS Team
Date: 2026-02-15
"""

from typing import Dict, List, Optional
from openai import OpenAI


class QueryAnalyzer:
    """æŸ¥è¯¢åˆ†æžå™¨ - æ™ºèƒ½åˆ¤æ–­æ£€ç´¢ç­–ç•¥"""
    
    def __init__(self, llm_client: OpenAI):
        self.llm = llm_client
    
    def needs_retrieval(self, query: str, module: str = "User Insights") -> Dict:
        """
        åˆ¤æ–­é—®é¢˜æ˜¯å¦éœ€è¦æ£€ç´¢ï¼ˆæ™ºèƒ½åˆ†æµï¼‰
        
        Returns:
            {
                "requires_retrieval": bool,
                "query_category": "greeting|meta|domain",
                "direct_response": Optional[str],
                "reasoning": str
            }
        """
        
        # ä½¿ç”¨ LLM å¿«é€Ÿåˆ¤æ–­ï¼ˆè½»é‡çº§æ¨¡åž‹ï¼‰
        prompt = f"""åˆ¤æ–­ä»¥ä¸‹ç”¨æˆ·é—®é¢˜æ˜¯å¦éœ€è¦æ£€ç´¢æ•°æ®åº“ã€‚

**ç”¨æˆ·é—®é¢˜**: {query}

**åˆ†ç±»æ ‡å‡†**:
- **greeting**: çº¯é—®å€™è¯­ï¼ˆå¦‚"ä½ å¥½"ã€"hi"ã€"æ—©ä¸Šå¥½"ï¼‰â†’ ä¸éœ€è¦æ£€ç´¢
- **meta**: å…³äºŽç³»ç»ŸåŠŸèƒ½çš„é—®é¢˜ï¼ˆå¦‚"ä½ èƒ½åšä»€ä¹ˆ"ã€"å¦‚ä½•ä½¿ç”¨"ã€"è¿™ä¸ªç³»ç»Ÿæ˜¯å¹²ä»€ä¹ˆçš„"ï¼‰â†’ ä¸éœ€è¦æ£€ç´¢
- **domain**: å…³äºŽç”µåŠ¨æ±½è½¦é¢†åŸŸçš„ä¸“ä¸šé—®é¢˜ â†’ **å¿…é¡»æ£€ç´¢**

**é‡è¦**: ä»¥ä¸‹é—®é¢˜å±žäºŽ domainï¼Œéœ€è¦æ£€ç´¢ï¼š
- è¯¢é—®ç”¨æˆ·ç±»åž‹ã€ç”¨æˆ·ç”»åƒã€ç”¨æˆ·éœ€æ±‚ï¼ˆå¦‚"æœ‰å“ªäº›ç”¨æˆ·ç±»åž‹"ï¼‰
- è¯¢é—®è½¦åž‹å‚æ•°ã€è¯„ä»·ã€å¯¹æ¯”ï¼ˆå¦‚"Model Y æ€Žä¹ˆæ ·"ï¼‰
- è¯¢é—®åŠŸèƒ½ã€åœºæ™¯ã€ç—›ç‚¹ï¼ˆå¦‚"æ™ºèƒ½åº§èˆ±è¯„ä»·"ï¼‰
- éœ€è¦æ’°å†™æ–‡æ¡£ã€åˆ†æžæ•°æ®çš„ä»»åŠ¡

è¿”å›ž JSON:
{{
    "category": "greeting|meta|domain",
    "requires_retrieval": true|false,
    "reasoning": "ç®€çŸ­è¯´æ˜Ž"
}}

åªè¿”å›ž JSONã€‚"""
        
        try:
            response = self.llm.chat(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=200,
                response_format={"type": "json_object"}
            )
            
            import json
            result = json.loads(response.choices[0].message.content)
            
            category = result.get("category", "domain")
            requires_retrieval = result.get("requires_retrieval", True)
            
            # äºŒæ¬¡éªŒè¯ï¼šåŒ…å«å…³é”®è¯çš„å¿…é¡»æ£€ç´¢
            domain_keywords = ["ç”¨æˆ·", "è½¦åž‹", "è¯„ä»·", "å¯¹æ¯”", "åˆ†æž", "ç”»åƒ", "éœ€æ±‚", "ç—›ç‚¹", "ç«žå“", "PRD", "Model", "ç‰¹æ–¯æ‹‰", "æ¯”äºšè¿ª", "ç†æƒ³", "è”šæ¥", "å°é¹"]
            if any(kw in query for kw in domain_keywords):
                requires_retrieval = True
                category = "domain"
            
            # ç”Ÿæˆç›´æŽ¥å›žå¤ï¼ˆå¦‚æžœä¸éœ€è¦æ£€ç´¢ï¼‰
            direct_response = None
            if not requires_retrieval:
                if category == "greeting":
                    direct_response = "ä½ å¥½ï¼æˆ‘æ˜¯ EV PM DSS æ™ºèƒ½åŠ©æ‰‹ã€‚\n\næˆ‘å¯ä»¥å¸®æ‚¨ï¼š\n- ðŸ“Š åˆ†æžç”¨æˆ·æ´žå¯Ÿå’Œéœ€æ±‚\n- âš”ï¸ è¿›è¡Œç«žå“å¯¹æ¯”åˆ†æž\n- ðŸ“ æ’°å†™äº§å“éœ€æ±‚æ–‡æ¡£\n\nè¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£ä»€ä¹ˆï¼Ÿ"
                elif category == "meta":
                    # ä½¿ç”¨ LLM åŸºäºŽç³»ç»Ÿæ–‡æ¡£ç”Ÿæˆå›žç­”
                    direct_response = self._generate_meta_response(query, module)
            
            return {
                "requires_retrieval": requires_retrieval,
                "query_category": category,
                "direct_response": direct_response,
                "reasoning": result.get("reasoning", "")
            }
            
        except Exception as e:
            # å¤±è´¥æ—¶ä¿å®ˆç­–ç•¥ï¼šå‡è®¾éœ€è¦æ£€ç´¢
            return {
                "requires_retrieval": True,
                "query_category": "domain",
                "direct_response": None,
                "reasoning": f"åˆ†ç±»å¤±è´¥ï¼Œé»˜è®¤éœ€è¦æ£€ç´¢: {str(e)}"
            }
    
    def _generate_meta_response(self, query: str, module: str = "User Insights") -> str:
        """
        åŸºäºŽç³»ç»Ÿæ–‡æ¡£ç”Ÿæˆ meta é—®é¢˜çš„å›žç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            module: å½“å‰æ¨¡å—
        
        Returns:
            LLMç”Ÿæˆçš„å›žç­”
        """
        # è¯»å–ç³»ç»Ÿæ–‡æ¡£
        import os
        from pathlib import Path
        
        doc_path = Path(__file__).parent.parent / "SYSTEM_INTRO.md"
        
        try:
            with open(doc_path, 'r', encoding='utf-8') as f:
                system_doc = f.read()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å–ç³»ç»Ÿæ–‡æ¡£: {e}")
            # é™çº§åˆ°ç®€å•å›žå¤
            return f"æˆ‘æ˜¯ EV PM DSS æ™ºèƒ½åŠ©æ‰‹ï¼Œå½“å‰åœ¨ {module} æ¨¡å—ã€‚æˆ‘å¯ä»¥å¸®æ‚¨åˆ†æžç”µåŠ¨æ±½è½¦ç”¨æˆ·éœ€æ±‚ã€ç«žå“å¯¹æ¯”å’Œæ’°å†™ PRDã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ"
        
        prompt = f"""ä½ æ˜¯ EV PM DSS çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œç”¨æˆ·åœ¨ {module} æ¨¡å—è¯¢é—®ç³»ç»ŸåŠŸèƒ½ã€‚

**ç³»ç»Ÿæ–‡æ¡£**:
{system_doc}

**ç”¨æˆ·é—®é¢˜**: {query}

**å›žç­”è¦æ±‚**:
1. åŸºäºŽç³»ç»Ÿæ–‡æ¡£å›žç­”ï¼Œä¸è¦æ·»åŠ æ–‡æ¡£ä¸­æ²¡æœ‰çš„åŠŸèƒ½
2. å¦‚æžœé—®é¢˜è¶…å‡ºç³»ç»ŸèŒƒå›´ï¼ˆéžç”µåŠ¨æ±½è½¦é¢†åŸŸï¼‰ï¼Œç¤¼è²Œæ‹’ç»å¹¶è¯´æ˜Žç³»ç»Ÿå®šä½
3. çªå‡ºå½“å‰æ¨¡å— ({module}) çš„èƒ½åŠ›
4. ç®€æ´ä¸“ä¸šï¼Œ2-3æ®µå³å¯
5. ä½¿ç”¨ Markdown æ ¼å¼ï¼Œå¯ä»¥ç”¨åˆ—è¡¨å’ŒåŠ ç²—

è¯·å›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""

        try:
            response = self.llm.chat(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,  # ç¨é«˜æ¸©åº¦ï¼Œæ›´è‡ªç„¶
                max_tokens=500
            )
            
            return response.choices[0].message.content.strip()
        
        except Exception as e:
            print(f"âš ï¸ LLM ç”Ÿæˆ meta å›žç­”å¤±è´¥: {e}")
            return f"æˆ‘æ˜¯ EV PM DSS æ™ºèƒ½åŠ©æ‰‹ï¼Œå½“å‰åœ¨ **{module}** æ¨¡å—ã€‚æˆ‘å¯ä»¥å¸®æ‚¨åˆ†æžç”µåŠ¨æ±½è½¦ç”¨æˆ·éœ€æ±‚ã€ç«žå“å¯¹æ¯”å’Œæ’°å†™ PRDã€‚è¯·å…·ä½“å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£ä»€ä¹ˆï¼Ÿ"
    
    def _get_module_help(self, module: str = "User Insights") -> str:
        """èŽ·å–å½“å‰æ¨¡å—çš„å¸®åŠ©ä¿¡æ¯"""
        
        help_texts = {
            "User Insights": """**EV PM DSS åŠŸèƒ½è¯´æ˜Ž**

å½“å‰æ‚¨åœ¨ **User Insights** æ¨¡å—ï¼Œå¯ä»¥ï¼š
- åˆ†æžç‰¹å®šç”¨æˆ·ç¾¤ä½“çš„éœ€æ±‚å’Œç—›ç‚¹
- äº†è§£ç”¨æˆ·å¯¹æŸä¸ªåŠŸèƒ½/è½¦åž‹çš„è¯„ä»·
- è¯†åˆ«ç”¨æˆ·ç”»åƒå’Œä½¿ç”¨åœºæ™¯

**ç¤ºä¾‹é—®é¢˜**:
- "æœ‰å“ªäº›ç”¨æˆ·ç±»åž‹ï¼Ÿ"
- "ç”¨æˆ·å¯¹æ™ºèƒ½åº§èˆ±æœ‰ä»€ä¹ˆè¯„ä»·ï¼Ÿ"
- "ç»­èˆªç„¦è™‘ä¸»è¦åœ¨å“ªäº›åœºæ™¯ï¼Ÿ"

åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å—å¯æŸ¥çœ‹æ›´å¤šåŠŸèƒ½ã€‚""",
            
            "Competitor Analysis": """**EV PM DSS åŠŸèƒ½è¯´æ˜Ž**

å½“å‰æ‚¨åœ¨ **Competitor Analysis** æ¨¡å—ï¼Œå¯ä»¥ï¼š
- å¯¹æ¯”ä¸åŒè½¦åž‹çš„å‚æ•°å’Œæ€§èƒ½
- åˆ†æžç«žå“çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿
- åŸºäºŽçœŸå®žç”¨æˆ·è¯„ä»·ç”Ÿæˆ SWOT åˆ†æž

**ç¤ºä¾‹é—®é¢˜**:
- "Model Y å’Œç†æƒ³ L7 å¯¹æ¯”"
- "æ¯”äºšè¿ªæµ·è±¹çš„ç«žäº‰åŠ›å¦‚ä½•ï¼Ÿ"
- "15-20ä¸‡çº¯ç”µ SUV å¸‚åœºåˆ†æž"

åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å—å¯æŸ¥çœ‹æ›´å¤šåŠŸèƒ½ã€‚""",
            
            "PRD Writer": """**EV PM DSS åŠŸèƒ½è¯´æ˜Ž**

å½“å‰æ‚¨åœ¨ **PRD Writer** æ¨¡å—ï¼Œå¯ä»¥ï¼š
- åŸºäºŽç”¨æˆ·ç”»åƒå’Œå¸‚åœºæ•°æ®æ’°å†™ PRD
- è‡ªåŠ¨ç”ŸæˆåŠŸèƒ½éœ€æ±‚å’Œä¼˜å…ˆçº§
- æä¾›æ•°æ®é©±åŠ¨çš„äº§å“å»ºè®®

**ç¤ºä¾‹é—®é¢˜**:
- "æ’°å†™æ™ºèƒ½åº§èˆ± PRD"
- "é’ˆå¯¹å¹´è½»å®¶åº­ç”¨æˆ·çš„è½¦åž‹éœ€æ±‚æ–‡æ¡£"
- "ç»­èˆªä¼˜åŒ–åŠŸèƒ½ PRD"

åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å—å¯æŸ¥çœ‹æ›´å¤šåŠŸèƒ½ã€‚"""
        }
    
    def extract_entities(self, query: str, module: str = "Competitor Analysis") -> Dict:
        """
        ä»Žç”¨æˆ·é—®é¢˜ä¸­æå–å“ç‰Œå’Œè½¦åž‹å®žä½“
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            module: å½“å‰æ¨¡å—
        
        Returns:
            {
                "brands": List[str],  # æå–çš„å“ç‰Œåˆ—è¡¨
                "models": List[str],  # æå–çš„è½¦åž‹åˆ—è¡¨
                "series": List[str],  # æå–çš„è½¦ç³»åˆ—è¡¨
                "extraction_confidence": float  # æå–ç½®ä¿¡åº¦
            }
        """
        
        prompt = f"""ä»Žä»¥ä¸‹ç”¨æˆ·é—®é¢˜ä¸­æå–ç”µåŠ¨æ±½è½¦ç›¸å…³çš„å“ç‰Œã€è½¦ç³»å’Œè½¦åž‹ä¿¡æ¯ã€‚

**ç”¨æˆ·é—®é¢˜**: {query}

**æ•°æ®åº“ä¸­çš„æ ‡å‡†å“ç‰Œåç§°**ï¼ˆä½ å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹åç§°ï¼Œä¸èƒ½ä½¿ç”¨ç®€ç§°æˆ–åˆ«åï¼‰:
- ç‰¹æ–¯æ‹‰ï¼ˆTeslaï¼‰
- æ¯”äºšè¿ªï¼ˆBYDï¼‰
- ç†æƒ³æ±½è½¦ï¼ˆç†æƒ³ / Li Autoï¼‰â†’ å¿…é¡»è¾“å‡º"ç†æƒ³æ±½è½¦"
- è”šæ¥ï¼ˆNIOï¼‰
- å°é¹ï¼ˆXpengï¼‰
- å°ç±³æ±½è½¦ï¼ˆå°ç±³ / Xiaomiï¼‰â†’ å¿…é¡»è¾“å‡º"å°ç±³æ±½è½¦"
- AITO é—®ç•Œï¼ˆé—®ç•Œ / AITOï¼‰â†’ å¿…é¡»è¾“å‡º"AITO é—®ç•Œ"
- æžæ°ªï¼ˆZeekrï¼‰
- å¥¥è¿ªï¼ˆAudiï¼‰
- å®é©¬ï¼ˆBMWï¼‰
- å¥”é©°ï¼ˆBenz / Mercedesï¼‰
- æ²ƒå°”æ²ƒï¼ˆVolvoï¼‰

**è½¦åž‹/è½¦ç³»ç¤ºä¾‹**:
- Model Y, Model 3, Model S, Model X
- æµ·è±¹, æ±‰, å”, å…ƒ Plus
- ç†æƒ³ L7, ç†æƒ³ L8, ç†æƒ³ L9
- ES6, ES8, ET5, ET7
- P7, G9, P5
- SU7

è¿”å›ž JSON æ ¼å¼ï¼š
{{
    "brands": ["å“ç‰Œ1", "å“ç‰Œ2"],  // å¿…é¡»ä½¿ç”¨ä¸Šé¢åˆ—å‡ºçš„æ ‡å‡†å“ç‰Œåç§°
    "models": ["è½¦åž‹1", "è½¦åž‹2"],  // æå–çš„å…·ä½“è½¦åž‹
    "series": ["è½¦ç³»1"],           // æå–çš„è½¦ç³»
    "extraction_confidence": 0.0-1.0  // æå–ç½®ä¿¡åº¦
}}

**è§„åˆ™**:
1. brands å­—æ®µå¿…é¡»ä½¿ç”¨ä¸Šé¢åˆ—å‡ºçš„æ ‡å‡†å“ç‰Œåç§°ï¼Œä¾‹å¦‚ç”¨æˆ·è¯´"å°ç±³"ä½ å¿…é¡»è¾“å‡º"å°ç±³æ±½è½¦"ï¼Œç”¨æˆ·è¯´"ç†æƒ³"ä½ å¿…é¡»è¾“å‡º"ç†æƒ³æ±½è½¦"ï¼Œç”¨æˆ·è¯´"é—®ç•Œ"ä½ å¿…é¡»è¾“å‡º"AITO é—®ç•Œ"
2. è½¦åž‹è¦åŒ…å«å“ç‰Œå‰ç¼€ï¼Œå¦‚ "ç‰¹æ–¯æ‹‰ Model Y"
3. å¦‚æžœé—®é¢˜ä¸­æ²¡æœ‰æåˆ°ä»»ä½•å“ç‰Œæˆ–è½¦åž‹ï¼Œè¿”å›žç©ºåˆ—è¡¨
4. ç½®ä¿¡åº¦: æ˜Žç¡®æåˆ°å“ç‰Œ=1.0, ä»…æš—ç¤º=0.5, æœªæåˆ°=0.0

åªè¿”å›ž JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        try:
            print(f"\nðŸ” [å®žä½“æå–] å¼€å§‹æå–å“ç‰Œå’Œè½¦åž‹...")
            print(f"   æŸ¥è¯¢: {query}")
            
            response = self.llm.chat(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=300,
                response_format={"type": "json_object"}
            )
            
            import json
            result = json.loads(response.choices[0].message.content)
            
            # éªŒè¯å’Œæ¸…ç†ç»“æžœ
            brands = result.get("brands", [])
            models = result.get("models", [])
            series = result.get("series", [])
            confidence = result.get("extraction_confidence", 0.0)
            
            # åŽ»é‡
            normalized_brands = list(dict.fromkeys(brands))
            
            print(f"   âœ… æå–ç»“æžœ: brands={normalized_brands}, models={models}, series={series}, confidence={confidence:.2f}")
            
            return {
                "brands": normalized_brands,
                "models": models,
                "series": series,
                "extraction_confidence": confidence
            }
            
        except Exception as e:
            print(f"   âŒ å®žä½“æå–å¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶è¿”å›žç©ºç»“æžœ
            return {
                "brands": [],
                "models": [],
                "series": [],
                "extraction_confidence": 0.0
            }
    
    
    def analyze_query(self, query: str, module: str) -> Dict:
        """
        åˆ†æžç”¨æˆ·é—®é¢˜ï¼Œè¿”å›žæ£€ç´¢ç­–ç•¥
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            module: å½“å‰æ¨¡å—ï¼ˆUser Insights / Competitor Analysis / PRD Writerï¼‰
        
        Returns:
            {
                "complexity": "simple|medium|complex",
                "query_type": "factual|analytical|creative",
                "data_sources": ["vector", "graph"],
                "n_results": int,
                "requires_rerank": bool,
                "reasoning": str
            }
        """
        
        # ä½¿ç”¨è½»é‡çº§æ¨¡åž‹ï¼ˆRouting Modelï¼‰è¿›è¡Œå¿«é€Ÿåˆ†æž
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„æŸ¥è¯¢åˆ†æžç³»ç»Ÿã€‚åˆ†æžä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œç¡®å®šæœ€ä¼˜æ£€ç´¢ç­–ç•¥ã€‚

**å½“å‰æ¨¡å—**: {module}
**ç”¨æˆ·é—®é¢˜**: {query}

è¯·åˆ†æžå¹¶è¿”å›ž JSON æ ¼å¼çš„ç»“æžœï¼š

{{
    "complexity": "simple|medium|complex",  // é—®é¢˜å¤æ‚åº¦
    "query_type": "factual|analytical|creative",  // é—®é¢˜ç±»åž‹
    "data_sources": ["vector", "graph"],  // éœ€è¦çš„æ•°æ®æº
    "n_results": 5-15,  // å»ºè®®æ£€ç´¢ç»“æžœæ•°é‡
    "requires_rerank": true|false,  // æ˜¯å¦éœ€è¦é‡æŽ’åº
    "reasoning": "ç®€çŸ­è¯´æ˜Žç†ç”±"
}}

**åˆ¤æ–­æ ‡å‡†**:
- **simple**: ç®€å•äº‹å®žæŸ¥è¯¢ï¼ˆå¦‚"Model Y ä»·æ ¼"ï¼‰â†’ vector, n_results=5
- **medium**: éœ€è¦å¯¹æ¯”åˆ†æžï¼ˆå¦‚"Model Y å’Œç†æƒ³L7 å¯¹æ¯”"ï¼‰â†’ vector+graph, n_results=10
- **complex**: éœ€è¦æ·±åº¦åˆ†æžï¼ˆå¦‚"æ’°å†™æ™ºèƒ½åº§èˆ± PRD"ï¼‰â†’ vector+graph+personas, n_results=15, rerank=true

åªè¿”å›ž JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        try:
            response = self.llm.chat(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=300,
                response_format={"type": "json_object"}  # å¼ºåˆ¶ JSON è¾“å‡º
            )
            
            import json
            analysis = json.loads(response.choices[0].message.content)
            
            # éªŒè¯å’Œé»˜è®¤å€¼
            analysis.setdefault("complexity", "medium")
            analysis.setdefault("query_type", "analytical")
            analysis.setdefault("data_sources", ["vector", "graph"])
            analysis.setdefault("n_results", 10)
            analysis.setdefault("requires_rerank", False)
            analysis.setdefault("reasoning", "è‡ªåŠ¨åˆ†æž")
            
            return analysis
            
        except Exception as e:
            # å¤±è´¥æ—¶è¿”å›žé»˜è®¤ç­–ç•¥
            return {
                "complexity": "medium",
                "query_type": "analytical",
                "data_sources": ["vector", "graph"],
                "n_results": 10,
                "requires_rerank": False,
                "reasoning": f"åˆ†æžå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: {str(e)}"
            }
    
    def get_retrieval_config(self, analysis: Dict, module: str) -> Dict:
        """
        æ ¹æ®åˆ†æžç»“æžœç”Ÿæˆæ£€ç´¢é…ç½®
        
        Returns:
            {
                "use_vector": bool,
                "use_graph": bool,
                "vector_n_results": int,
                "graph_queries": List[str],
                "enable_rerank": bool
            }
        """
        config = {
            "use_vector": "vector" in analysis["data_sources"],
            "use_graph": "graph" in analysis["data_sources"],
            "vector_n_results": analysis["n_results"],
            "enable_rerank": analysis["requires_rerank"],
            "complexity": analysis["complexity"]
        }
        
        # æ ¹æ®æ¨¡å—è°ƒæ•´é…ç½®
        if module == "User Insights":
            config["use_graph"] = True  # å§‹ç»ˆä½¿ç”¨ Persona
        elif module == "PRD Writer":
            config["use_vector"] = True
            config["use_graph"] = True
            config["vector_n_results"] = max(15, config["vector_n_results"])
        
        return config


class RetrievalQualityChecker:
    """æ£€ç´¢è´¨é‡æŠŠæŽ§æ¨¡å—"""
    
    def __init__(self, llm_client: OpenAI):
        self.llm = llm_client
    
    def check_relevance(
        self, 
        query: str, 
        retrieved_docs: List[Dict],
        min_relevant_ratio: float = 0.4
    ) -> Dict:
        """
        æ£€æŸ¥æ£€ç´¢ç»“æžœä¸Žé—®é¢˜çš„ç›¸å…³åº¦
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
            min_relevant_ratio: æœ€ä½Žç›¸å…³æ–‡æ¡£æ¯”ä¾‹é˜ˆå€¼
        
        Returns:
            {
                "is_sufficient": bool,
                "relevant_count": int,
                "total_count": int,
                "relevance_ratio": float,
                "suggestion": str,
                "should_reretrieve": bool
            }
        """
        
        if not retrieved_docs:
            return {
                "is_sufficient": False,
                "relevant_count": 0,
                "total_count": 0,
                "relevance_ratio": 0.0,
                "suggestion": "æœªæ£€ç´¢åˆ°ä»»ä½•æ–‡æ¡£ï¼Œéœ€è¦é‡æ–°æ£€ç´¢",
                "should_reretrieve": True
            }
        
        # æž„å»ºæ£€æŸ¥ prompt
        docs_preview = "\n\n".join([
            f"æ–‡æ¡£ {i+1}:\n{doc.get('text', '')[:200]}..."
            for i, doc in enumerate(retrieved_docs[:15])  # æ£€æŸ¥å‰ 15 ä¸ªæ–‡æ¡£ï¼ˆé€‚åº”æ›´å¤§æ•°æ®é›†ï¼‰
        ])
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢è´¨é‡è¯„ä¼°ç³»ç»Ÿã€‚åˆ¤æ–­æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸Žç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§ã€‚

**ç”¨æˆ·é—®é¢˜**: {query}

**æ£€ç´¢åˆ°çš„æ–‡æ¡£**:
{docs_preview}

è¯·è¯„ä¼°å¹¶è¿”å›ž JSONï¼š

{{
    "relevant_count": 0-5,  // ç›¸å…³æ–‡æ¡£æ•°é‡
    "relevance_scores": [0.0-1.0, ...],  // æ¯ä¸ªæ–‡æ¡£çš„ç›¸å…³åº¦è¯„åˆ†
    "is_sufficient": true|false,  // æ˜¯å¦è¶³å¤Ÿå›žç­”é—®é¢˜
    "suggestion": "è¯„ä¼°è¯´æ˜Ž"
}}

**åˆ¤æ–­æ ‡å‡†**:
- ç›¸å…³åº¦ > 0.7: é«˜åº¦ç›¸å…³
- ç›¸å…³åº¦ 0.4-0.7: ä¸­åº¦ç›¸å…³
- ç›¸å…³åº¦ < 0.4: ä¸ç›¸å…³

åªè¿”å›ž JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        try:
            response = self.llm.chat(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=300,
                response_format={"type": "json_object"}
            )
            
            import json
            result = json.loads(response.choices[0].message.content)
            
            relevant_count = result.get("relevant_count", 0)
            total_count = len(retrieved_docs)
            relevance_ratio = relevant_count / total_count if total_count > 0 else 0.0
            
            is_sufficient = result.get("is_sufficient", False)
            should_reretrieve = relevance_ratio < min_relevant_ratio or not is_sufficient
            
            return {
                "is_sufficient": is_sufficient,
                "relevant_count": relevant_count,
                "total_count": total_count,
                "relevance_ratio": relevance_ratio,
                "suggestion": result.get("suggestion", ""),
                "should_reretrieve": should_reretrieve,
                "relevance_scores": result.get("relevance_scores", [])
            }
            
        except Exception as e:
            # å¤±è´¥æ—¶ä¿å®ˆç­–ç•¥ï¼šæŠ¥å‘Šå¤±è´¥ï¼Œè¦æ±‚é‡æ–°æ£€ç´¢
            return {
                "is_sufficient": False,  # æ”¹ä¸º Falseï¼Œä¸å‡è®¾ç»“æžœå¯ç”¨
                "relevant_count": 0,
                "total_count": len(retrieved_docs),
                "relevance_ratio": 0.0,
                "suggestion": f"è´¨é‡æ£€æŸ¥å¤±è´¥ï¼Œå»ºè®®é‡æ–°æ£€ç´¢æˆ–æ‰©å¤§èŒƒå›´: {str(e)}",
                "should_reretrieve": True  # æ”¹ä¸º Trueï¼Œå»ºè®®é‡æ–°æ£€ç´¢
            }
    
    def suggest_refinement(self, query: str, quality_result: Dict) -> str:
        """å»ºè®®å¦‚ä½•æ”¹è¿›æ£€ç´¢"""
        
        if quality_result["should_reretrieve"]:
            suggestions = []
            
            if quality_result["relevance_ratio"] < 0.2:
                suggestions.append("æ‰©å¤§æ£€ç´¢èŒƒå›´ï¼ˆå¢žåŠ  n_resultsï¼‰")
                suggestions.append("å°è¯•æ”¹å†™æŸ¥è¯¢å…³é”®è¯")
            elif quality_result["relevance_ratio"] < 0.4:
                suggestions.append("è°ƒæ•´æ£€ç´¢ç­–ç•¥ï¼ˆå¦‚å¢žåŠ å›¾æ•°æ®åº“æŸ¥è¯¢ï¼‰")
            
            return "; ".join(suggestions) if suggestions else "å»ºè®®é‡æ–°æ£€ç´¢"
        
        return "æ£€ç´¢è´¨é‡è‰¯å¥½"
